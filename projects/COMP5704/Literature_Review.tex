
% ===========================================================================
% Title:
% ---------------------------------------------------------------------------
% to create Type I fonts type "dvips -P cmz -t letter <filename>"
% ===========================================================================
\documentclass[11pt]{article}       %--- LATEX 2e base
\usepackage{latexsym}               %--- LATEX 2e base
%---------------- Wide format -----------------------------------------------
\textwidth=6in \textheight=9in \oddsidemargin=0.25in
\evensidemargin=0.25in \topmargin=-0.5in
%--------------- Def., Theorem, Proof, etc. ---------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}
\newenvironment{proof}           {\noindent{\bf Proof.} }%
                                 {\null\hfill$\Box$\par\medskip}
%--------------- Algorithm --------------------------------------------------
\newtheorem{algX}{Algorithm}
\newenvironment{algorithm}       {\begin{algX}\begin{em}}%
                                 {\par\noindent --- End of Algorithm ---
                                 \end{em}\end{algX}}
\newcommand{\step}[2]            {\begin{list}{}
                                  {  \setlength{\topsep}{0cm}
                                     \setlength{\partopsep}{0cm}
                                     \setlength{\leftmargin}{0.8cm}
                                     \setlength{\labelwidth}{0.7cm}
                                     \setlength{\labelsep}{0.1cm}    }
                                  \item[#1]#2    \end{list}}
                                 % usage: \begin{algorithm} \label{xyz}
                                 %        ... \step{(1)}{...} ...
                                 %        \end{algorithm}
%--------------- Figures ----------------------------------------------------
\usepackage{graphicx}

\newcommand{\includeFig}[3]      {\begin{figure}[htb] \begin{center}
                                 \includegraphics
                                 [width=4in,keepaspectratio] %comment this line to disable scaling
                                 {#2}\caption{\label{#1}#3} \end{center} \end{figure}}
                                 % usage: \includeFig{label}{file}{caption}


% ===========================================================================
\begin{document}
% ===========================================================================

% ############################################################################
% Title
% ############################################################################

\title{LITERATURE REVIEW: --- Parallelization between Layers of Convolutional Neural Networks to Create Faster 3D Object Reconstruction}


% ############################################################################
% Author(s) (no blank lines !)
\author{
% ############################################################################
Tansin Jahan\\
School of Computer Science\\
Carleton University\\
Ottawa, Canada K1S 5B6\\
{\em tansinjahan@cmail.carleton.ca}
% ############################################################################
} % end-authors
% ############################################################################
\date{October 5,2018}
\maketitle



% ############################################################################
\section{Introduction} \label{intro}
% ############################################################################

The idea of parallel computing infers to execute more than one tasks simultaneously so that the complexity(ex- time, space etc.) in computation can be carried out smoothly. Though this is the simplest definition of parallel computing, in the real world, parallelization means a lot more than just handling the complexity in the computation of algorithms. With the development of Deep Learning Networks, parallel computing has become the essential choice for the implementation of these huge networks. Therefore the project idea here is to develop the parallelism between convolutional layers of neural networks so that the time for computation with a large number of input images can be minimized.

Typically there are million of parameters(weights, biases) and a large amount of data involves in a CNN model. This large amount of data as input produces output at each layer which known as Forwarding propagation. The loss is calculated at the same time and updated parameters are backpropagated to the layers to minimize the loss. For this, the different optimization technique is applied such as Stochastic Gradient Descent or ADAM etc. This whole process is computationally expensive and therefore needs to be parallelized so that the training time minimizes and the network can run faster than usual times. 

% ############################################################################
\section{Literature Review} \label{litrev}
% ############################################################################

With the recent development of Convolutional Neural Network, it has been used to solve several Computer Vision problem. For example, object detection or reconstruction from input image, semantic information extraction from a scenario, object segmentation- these are all recent computer vision application where CNN has been used to produce better result. Likewise, 3D reconstruction of an object is one of the Computer Vision problem and recently multiple approaches (ex: 3D-GAN, 3D-shapenets) has been proposed as a solution to the problem \cite{dr1}. Previous work shows that given a depth image as input, the volumetric representation can be produced \cite{dr2}. Following these works, an approach to reconstruct 3D voxelized object from different viewpoint of one or more images of that object (i.e. single-view or multi-view) is proposed where recurrent neural network has been used \cite{dr7}. In total 50,000 model is used to train and test the proposed network. Training the network with this large amount of data is really time consuming and therefore introducing parallelism between the layers of the model can help to improve the performance of the network.

\subsection{GPU implementation} \label{GPUimpl}
GPU implementation for convolutional operation of deep networks has become very popular to improve state-of-the-art results. In the paper, Imagenet classification with deep convolutional neural networks - two GTX 580 3GB GPU has been used to train the network which took five to six days to complete the whole training \cite{dr5}. In this parallelization scheme, they kept half of the kernels (or neurons) on each GPU, where the GPUs communicate only in certain layers.


% ############################################################################
% Bibliography
% ############################################################################
\bibliographystyle{plain}
\bibliography{my-bibliography}     %loads my-bibliography.bib

% ============================================================================
\end{document}
% ============================================================================
