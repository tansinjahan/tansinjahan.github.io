
% ===========================================================================
% Title:
% ---------------------------------------------------------------------------
% to create Type I fonts type "dvips -P cmz -t letter <filename>"
% ===========================================================================
\documentclass[11pt]{article}       %--- LATEX 2e base
\usepackage{latexsym}               %--- LATEX 2e base
%---------------- Wide format -----------------------------------------------
\textwidth=6in \textheight=9in \oddsidemargin=0.25in
\evensidemargin=0.25in \topmargin=-0.5in
%--------------- Def., Theorem, Proof, etc. ---------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}
\newenvironment{proof}           {\noindent{\bf Proof.} }%
                                 {\null\hfill$\Box$\par\medskip}
%--------------- Algorithm --------------------------------------------------
\newtheorem{algX}{Algorithm}
\newenvironment{algorithm}       {\begin{algX}\begin{em}}%
                                 {\par\noindent --- End of Algorithm ---
                                 \end{em}\end{algX}}
\newcommand{\step}[2]            {\begin{list}{}
                                  {  \setlength{\topsep}{0cm}
                                     \setlength{\partopsep}{0cm}
                                     \setlength{\leftmargin}{0.8cm}
                                     \setlength{\labelwidth}{0.7cm}
                                     \setlength{\labelsep}{0.1cm}    }
                                  \item[#1]#2    \end{list}}
                                 % usage: \begin{algorithm} \label{xyz}
                                 %        ... \step{(1)}{...} ...
                                 %        \end{algorithm}
%--------------- Figures ----------------------------------------------------
\usepackage{graphicx}

\newcommand{\includeFig}[3]      {\begin{figure}[htb] \begin{center}
                                 \includegraphics
                                 [width=4in,keepaspectratio] %comment this line to disable scaling
                                 {#2}\caption{\label{#1}#3} \end{center} \end{figure}}
                                 % usage: \includeFig{label}{file}{caption}


% ===========================================================================
\begin{document}
% ===========================================================================

% ############################################################################
% Title
% ############################################################################

\title{LITERATURE REVIEW: --- Parallelizing the Interpolation between Latent Space of Autoencoder Networks to Introduce Novelty in 3D Object Reconstruction}


% ############################################################################
% Author(s) (no blank lines !)
\author{
% ############################################################################
Tansin Jahan\\
School of Computer Science\\
Carleton University\\
Ottawa, Canada K1S 5B6\\
{\em tansinjahan@cmail.carleton.ca}
% ############################################################################
} % end-authors
% ############################################################################
\date{October 5,2018}
\maketitle



% ############################################################################
\section{Introduction} \label{intro}
% ############################################################################

The idea of parallel computing infers to execute more than one tasks simultaneously so that the complexity(ex- time, space etc.) in computation can be carried out smoothly. Though this is the simplest definition of parallel computing, in the real world, parallelization means a lot more than just handling the complexity in the computation of algorithms. With the development of Deep Learning Networks, parallel computing has become the essential choice for the implementation of these huge networks.

This project involves parallelizing the vector addition produced by a simple \textbf{Autoencoder Neural Network}  which is popular to reconstruct same 3D object given as an input. Autoencoder, being a symmetrical deep network involves several convolutional layer and parameters for computation.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.5]{autoencoder-architecture.png}
\caption{A Simple Autoencoder Architecture}
\end{center}
\end{figure}
Typically this network feeds on 3D volumes involving higher dimension(i.e. 32x32x32) and then reduces that into lower dimension \cite{dr8} which is called as latent space(also Z). In latent space, represented as Z vector, the input object has minimum dimension with maximum features. From the lower dimension, the Z vector passes to deconvolution and therefore produces the same 3D volumes. But we can combine Z vector of two input volumes and interpolate new points as Z vector so that decoder can produce new 3D objects based on this interpolation. This interpolation of Z vector (addition of two Z vectors) can be an overhead for the performance of the Autoencoder. Let us consider we have 50 inputs. Then for each input, we have to compare it with another 49 inputs and calculate interpolation each time. So, for 50 inputs the vector addition will be 50 x 50 which is in total 2500 times addition of vectors.

The above calculation can take much time compared to the convolution and therefore we can parallelize this computation in GPU by using Multithreading option. In conclusion, for this project several thread will be introduced to compute vector addition in CUDA so that the performance of the whole network can be improved.    
 

% ############################################################################
\section{Literature Review} \label{litrev}
% ############################################################################

With the recent development of Convolutional Neural Network, it has been used to solve several Computer Vision problem. For example, object detection or reconstruction from input image, semantic information extraction from a scenario, object segmentation- these are all recent computer vision application where CNN has been used to produce better result. Likewise, 3D reconstruction of an object is one of the Computer Vision problem and recently multiple approaches (ex: 3D-GAN, 3D-shapenets) has been proposed as a solution to the problem \cite{dr1}. Previous work shows that given a depth image as input, the volumetric representation can be produced \cite{dr2}. Following these works, an approach to reconstruct 3D voxelized object from different viewpoint of one or more images of that object (i.e. single-view or multi-view) is proposed where recurrent neural network has been used \cite{dr7}. In total 50,000 model is used to train and test the proposed network. Training the network with this large amount of data is really time consuming and therefore introducing parallelism between the layers of the model can help to improve the performance of the network.

\subsection{GPU implementation} \label{GPUimpl}
GPUs are capable of efficiently running parallel programs and outperforms CPUs in terms of raw computing power. In the paper, 'Imagenet classification with deep convolutional neural networks' - two GTX 580 3GB GPU has been used to train the network which took five to six days to complete the whole training \cite{dr5}. In this parallelization scheme, they kept half of the kernels (or neurons) on each GPU, where the GPUs communicate only in certain layers.

Though a single GPU can greatly accelerate neural network training and testing, sometimes it is desirable
for muptiple GPUs to be used at once because the neural network model and data may not fit onto a single GPU or training with one GPU will simply take too long. Many researchers have approached the question of how to best use multiple GPUs to train deep networks more effectively. So, here I discuss one such method to parallelize convolutional neural networks described by Krizhevsky in his article ‘One weird trick for parallelizing convolutional neural networks’ \cite{dr4}. Many convolutional neural networks comprise both convolutional layers, where the linear function f(x) is a convolution of the input and a set of
learned weights, and fully connected layers, where the linear function f(x) is an inner product between the input and a set of learned weights. Krizhevsky’s key insight is that different types of parallelization are better for different types of layers. Convolutional layers generally require a large amount of computation, but have fewer parameters than fully connected layers. Thus, sharing updated parameters between GPUs is less burdensome than for fully connected layers, and splitting data amongst multiple GPUs helps speed computation. To this end, Krizevsky proposed parallelizing the convolutions with ‘data parallelization’, where the same convolutional layers are on each GPU, but the GPU process different batches of the data, and parallelizing the fully connected layers with ‘model parallelism’, where the model is split amongst GPUs, but each GPU sees the same data \cite{dr4}.


% ############################################################################
% Bibliography
% ############################################################################
\bibliographystyle{plain}
\bibliography{my-bibliography}     %loads my-bibliography.bib

% ============================================================================
\end{document}
% ============================================================================
